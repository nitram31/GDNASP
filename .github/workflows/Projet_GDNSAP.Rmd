---
title: "Untitled"
output: html_document
date: '2022-10-12'
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
library(jsonlite)
library(tidyverse)

# Generating the csv used to populate neo4j database

uniprot = read_tsv("data/uniprot.proteome.UP000000625.tab")

mapping = uniprot %>% 
  select(Entry, names = `Gene Names (ordered locus)`) %>%
  mutate(bnumber=str_extract(names, 'b\\d+')) %>%
  select(bnumber, uniprotID=Entry) %>%
  filter(!is.na(bnumber)) %>% # 2021 → P0DQD7 and P0A6D5 are lost (no bnumber)
  arrange(bnumber)



keywords = uniprot %>% 
  select(uniprotID=Entry, keyword=Keywords) %>%
  right_join(mapping, by="uniprotID") %>% # right join to remove those without bnumber
  separate_rows(keyword, sep=';') %>%
  select(bnumber, keyword) %>%
  arrange(bnumber)

ref_sets = keywords %>% 
  group_by(keyword) %>%
  summarise(count=n(), elements = list(bnumber)) %>%
  ungroup %>%
  filter(count>1)  %>%
  select(id=keyword, desc=count, elements)


interpro = uniprot %>% 
  select(uniprotID=Entry, keyword=InterPro) %>%
  right_join(mapping, by="uniprotID") %>% # right join to remove those without bnumber
  separate_rows(keyword, sep=';') %>%
  select(bnumber, keyword) %>%
  arrange(bnumber)%>%
  filter(!(keyword == "")) %>%
  write_csv("neo4j.import/interpro.csv")

inter_sets = interpro %>% 
  group_by(keyword) %>%
  summarise(count=n(), elements = list(bnumber), id = list(id)) %>%
  ungroup %>%
  filter(count>1)  %>%
  select(id=keyword, desc=count, elements) %>%
  write_csv("neo4j.import/inter_set.csv")


interpro = uniprot %>% 
  select(uniprotID=Entry, keyword=InterPro) %>%
  right_join(mapping, by="uniprotID") %>% # right join to remove those without bnumber
  separate_rows(keyword, sep=';') %>%
  select(bnumber, keyword) %>%
  arrange(bnumber)%>%
  filter(!(keyword == ""))

inter_sets = interpro %>% 
  group_by(keyword) %>%
  summarise(count=n(), elements = list(bnumber), id = list(id)) %>%
  ungroup %>%
  filter(count>1)  %>%
  select(id=keyword, desc=count, elements)


geneOn = uniprot %>% 
  select(uniprotID=Entry, keyword=`Gene Ontology IDs`) %>%
  right_join(mapping, by="uniprotID") %>% # right join to remove those without bnumber
  separate_rows(keyword, sep=';') %>%
  select(bnumber, keyword) %>%
  arrange(bnumber)%>%
  filter(!(keyword == ""))

geneOn_sets = geneOn %>% 
  group_by(keyword) %>%
  summarise(count=n(), elements = list(bnumber)) %>%
  ungroup %>%
  filter(count>1)  %>%
  select(id=keyword, desc=count, elements)


interpro = uniprot %>% 
  select(bnumber=strsplit(`Gene Names (ordered locus)`, ' ')[[1]][1], keyword=`Gene Ontology IDs`) %>%
  right_join(mapping, by="uniprotID") %>% # right join to remove those without bnumber
  separate_rows(keyword, sep=';') %>%
  select(bnumber, keyword) %>%
  arrange(bnumber)%>%
  filter(!(keyword == ""))


keywords %>% 
  select(keyword) %>%
  unique %>%
  write_csv("neo4j.import/uniprot.keywords.csv")


pmids = read_tsv("data/bnumber.PMID.tsv", col_types = "cc")
pmids %>% select(PMID) %>% unique %>% write_csv("neo4j.import/ncbi.pmid.csv")
pmids %>% rename(gene_id = bnumber) %>% write_csv("neo4j.import/ncbi.pmid.genes.csv")

keywords %>% write_csv("neo4j.import/uniprot.keywords.genes.csv")


path = read_tsv("data/pathwaysK-12-substr.-MG1655.txt", col_types = "cc")
path %>% select(Pathways) %>%  write_csv("neo4j.import/pathways.ids.csv")
path %>% write_csv("neo4j.import/pathways.csv")


geneK = read_tsv("data/genesK-12-substr.-MG1655.txt", col_types = "cc")
geneK_mapping = geneK %>% 
  select('Object ID', names = `Names`) %>%
  mutate(bnumber=str_extract(names, 'b\\d+')) %>%
  select(bnumber, gene_id='Object ID') %>%
  filter(!is.na(bnumber)) %>% # 2021 → P0DQD7 and P0A6D5 are lost (no bnumber)
  arrange(bnumber)

geneK_mapping %>% write_csv("neo4j.import/geneK_mapping.csv")

TU = read_tsv("data/all-transcription-units.txt")
TU %>% select(Site, Left, Right, Strand) %>% write_csv('neo4j.import/transcription_unit.csv')

alias = read_tsv("neo4j.import/mapping.bnumber_ncbi.tsv") %>% select(dbid, dbname) %>% write_csv("neo4j.import/alias.csv")

links.detailed = read_delim("data/511145.protein.links.detailed.v11.5.txt.gz", delim=" ", col_types = "ccnnnnnnnn")
links.detailed

links.detailed %>%
  filter(column>0 & protein1 < protein2) %>%
  select(protein1, protein2, coexpression, experimental, neighborhood, textmining, database, combined_score) %>%
  mutate(organism=str_extract(protein1, '^\\d+'), id1=str_extract(protein1, 'b\\d+'), id2=str_extract(protein2, 'b\\d+')) %>%
  select(organism:id2,coexpression, experimental, neighborhood, textmining, database, combined_score) %>%
  write_csv("neo4j.import/string.total.csv")
  
GOTerms = uniprot %>% 
  select(uniprotID=Entry, GOTerm=`Gene Ontology IDs`) %>%
  right_join(mapping) %>% # right join to remove those without bnumber
  separate_rows(GOTerm, sep='; ') %>%
  select(bnumber, GOTerm) %>%
  arrange(bnumber)

GOTerms %>% write_csv("neo4j.import/uniprot.GOTerm.bnumber.csv")

```
 
 
 
```{r}
if (!require('neo4r')) { # client neo4j 
  install.packages('neo4r')
}

## Loading required package: neo4r

library(neo4r)
neodb = neo4j_api$new(
  url = "http://localhost:7474", 
  user = "neo4j", 
  password = "Railgun31300"
)
cypher = function(query, neo4j=neodb, ...) call_neo4j(query=query, con=neo4j, ...)

# RESETING DATABASE
'MATCH ()-[r]-() DELETE(r)' %>% cypher
'MATCH ()-[r]-() RETURN count(r)' %>% cypher %>% unlist
'MATCH (n) DELETE(n)' %>% cypher
'MATCH (n) RETURN count(n)' %>% cypher %>% unlist


# Importing gene node
"
LOAD CSV WITH HEADERS FROM 'file:///gene.tsv' AS row FIELDTERMINATOR '\t' 
CREATE (n:Gene)
SET n = row,
 n.id = row.bnumber,
 n.organism = toInteger('511145'),
 n.rank = toInteger(row.rank),
 n.strand = row.strand,
 n.begin = toInteger(row.begin),
 n.end = toInteger(row.end)
"  %>% cypher

'MATCH (n:Gene) RETURN count(n)' %>% cypher %>% unlist

"MATCH (n:Gene) RETURN n LIMIT 6" %>% cypher %>% .$n

'CREATE INDEX ON :Gene(id)' %>% cypher

# Importing keywords node
'LOAD CSV WITH HEADERS FROM "file:///uniprot.keywords.csv" AS row 
CREATE (n:Keyword)
SET n = row,
 n.id = row.keyword
'  %>% cypher

'CREATE INDEX ON :Keyword(id)' %>% cypher

'MATCH (n:Keyword) RETURN count(n)' %>% cypher %>% unlist

"MATCH (:Keyword)-[r:describes]->(:Gene) DELETE r" %>% cypher

"
LOAD CSV WITH HEADERS FROM 'file:///uniprot.keywords.genes.csv' AS line 
MATCH (k:Keyword),(g:Gene) 
WHERE k.id=line.keyword AND g.id=line.bnumber
WITH k,g 
MERGE (k)-[:describes]->(g)
" %>% cypher

"MATCH (:Keyword)-[r:describes]->(:Gene) RETURN count(r)" %>% cypher %>% unlist

# Importing PubMed node
"MATCH (:PubMed)-[r:cites]->(:Gene) DELETE r" %>% cypher


'MATCH (n:PubMed) DELETE n' %>% cypher
'LOAD CSV WITH HEADERS FROM "file:///ncbi.pmid.csv" AS row 
CREATE (n:PubMed)
SET n = row,
 n.id = row.PMID
'  %>% cypher
'CREATE INDEX ON :PubMed(id)' %>% cypher

'MATCH (n:PubMed) RETURN count(n)' %>% cypher %>% unlist

"MATCH (:PubMed)-[r:cites]->(:Gene) DELETE r" %>% cypher
"
LOAD CSV WITH HEADERS FROM 'file:///ncbi.pmid.genes.csv' AS line 
MATCH (p:PubMed),(g:Gene) 
WHERE p.id=line.PMID AND g.id=line.gene_id
WITH p,g 
MERGE (p)-[:cites]->(g)
" %>% cypher

"MATCH (:PubMed)-[r:cites]->(:Gene) RETURN count(r)" %>% cypher %>% unlist

# Creating pathways node and linking them to genes
'MATCH (n:Pathways) DELETE n' %>% cypher

'LOAD CSV WITH HEADERS FROM "file:///pathways.ids.csv" AS row 
CREATE (n:Pathways)
SET n = row,
 n.id = row.Pathways
'  %>% cypher
'CREATE INDEX ON :Pathways(Pathways)' %>% cypher

'MATCH (n:Pathways) RETURN count(n)' %>% cypher %>% unlist

"
LOAD CSV WITH HEADERS FROM 'file:///mapping_gene_pathways.csv' AS line 
MATCH (p:Pathways),(g:Gene) 
WHERE p.id=line.pathway_id AND g.id=line.gene_id
WITH p,g 
MERGE (p)-[:requires]->(g)
" %>% cypher

"MATCH (n:Pathways)-[r:requires]->(:Gene) RETURN count(r)" %>% cypher %>% unlist

# Creating transcription units nodes 

'MATCH (n:TU) DELETE n' %>% cypher

'LOAD CSV WITH HEADERS FROM "file:///transcription_unit.csv" AS row 
CREATE (n:TU)
SET n = row,
 n.id = row.Site,
 n.Left = row.Left,
 n.Right = row.Right,
 n.Strand = row.Strand
'  %>% cypher
'CREATE INDEX ON :TU(TU)' %>% cypher

'MATCH (n:TU) RETURN count(n)' %>% cypher %>% unlist

"
LOAD CSV WITH HEADERS FROM 'file:///mapping_gene_TU.csv' AS line 
MATCH (p:TU),(g:Gene) 
WHERE p.id=line.TU_id AND g.id=line.gene_id
WITH p,g 
MERGE (p)-[:harbors]->(g)
" %>% cypher

"MATCH (n:TU)-[r:harbors]->(:Gene) RETURN count(r)" %>% cypher %>% unlist

# Creating interpro

'MATCH (n:InterPro) DELETE n' %>% cypher

'LOAD CSV WITH HEADERS FROM "file:///inter_set.csv" AS row 
CREATE (n:InterPro)
SET n = row,
 n.id = row.id,
 n.desc = row.desc,
 n.elements = row.elements
'  %>% cypher
'CREATE INDEX ON :InterPro(InterPro)' %>% cypher

'MATCH (n:InterPro) RETURN count(n)' %>% cypher %>% unlist

"
LOAD CSV WITH HEADERS FROM 'file:///interpro.csv' AS line 
MATCH (p:InterPro),(g:Gene) 
WHERE p.id=line.keyword AND g.id=line.bnumber
WITH p,g 
MERGE (p)-[:harbored_by]->(g)
" %>% cypher

"MATCH (n:InterPro)-[r:harbored_by]->(:Gene) RETURN count(r)" %>% cypher %>% unlist


"MATCH (n:Pathway) RETURN count(n)" %>% cypher %>% unlist

# Creating Alias node

"MATCH (n:Alias) delete n" %>% cypher 
"LOAD CSV WITH HEADERS FROM 'file:///alias.csv' AS row
CREATE (n:Alias)
SET n = row,
 n.dbid = row.dbid,
 n.dbsource = row.dbsource,
 n.organism = 511145
"  %>% cypher


"
LOAD CSV WITH HEADERS FROM 'file:///mapping.bnumber_ncbi.tsv' AS line FIELDTERMINATOR '\t' 
MATCH (p:Alias),(g:Gene) 
WHERE p.dbid=line.dbid AND g.id=line.bnumber 
WITH p,g 
MERGE (p)-[:refers_to]->(g)
" %>% cypher



"MATCH (n:Alias) RETURN count(n)" %>% cypher %>% unlist






```

```{r}
# Creating GOTerms
"MATCH (:GOTerm)-[r]-() DELETE r" %>% cypher
"MATCH (n:GOTerm) DELETE n" %>% cypher
"
LOAD CSV WITH HEADERS FROM 'file:///go.nodes.tsv' AS row FIELDTERMINATOR '\t' 
CREATE (n:GOTerm)
SET n.id = row.id,
n.name = row.desc,
n.desc  = row.def,
n.namespace = row.namespace
"  %>% cypher

"MATCH (n:GOTerm) RETURN count(n)" %>% cypher %>% unlist

"MATCH (n:GOTerm) RETURN n LIMIT 6" %>% cypher %>% .$n

"
LOAD CSV WITH HEADERS FROM 'file:///go.is_a.edges.tsv' AS line FIELDTERMINATOR '\t' 
MATCH (t1:GOTerm),(t2:GOTerm) 
WHERE t1.id=line.term1 AND t2.id=line.term2
WITH t1,t2 
MERGE (t2)-[r:is_a]->(t1)
" %>% cypher


"MATCH ()-[r:is_a]->() RETURN count(r)" %>% cypher %>% unlist

"
LOAD CSV WITH HEADERS FROM 'file:///go.part_of.edges.tsv' AS line FIELDTERMINATOR '\t' 
MATCH (t1:GOTerm),(t2:GOTerm) 
WHERE t1.id=line.term1 AND t2.id=line.term2
WITH t1,t2 
MERGE (t2)-[r:part_of]->(t1)
" %>% cypher

"MATCH ()-[r:part_of]->() RETURN count(r)" %>% cypher %>% unlist


"
LOAD CSV WITH HEADERS FROM 'file:///uniprot.GOTerm.bnumber.csv' AS line
MATCH (t:GOTerm),(g:Gene) 
WHERE t.id=line.GOTerm AND g.id=line.bnumber
WITH t,g
MERGE (t)-[:annotates]->(g)
" %>% cypher

"MATCH ()-[r:annotates]->() RETURN count(r)" %>% cypher %>% unlist

# Creating STRINGdb relationships

'MATCH ()-[r:STRINGdb]-() delete r' %>% cypher %>% unlist
'
LOAD CSV WITH HEADERS FROM "file:///string.total.csv" AS line
MATCH (g1:Gene),(g2:Gene)
WHERE g1.id=line.id1 AND g2.id=line.id2
WITH g1,g2, toInteger(line.coexpression) as coexpression, toInteger(line.experimental) as experimental, 
toInteger(line.neighborhood) as neighborhood, toInteger(line.textmining) as textmining, 
toInteger(line.database) as database, toInteger(line.combined_score) AS combined_score
MERGE (g1)-[r:STRINGdb]-(g2)
SET r.coexpression = coexpression, r.experimental = experimental, r.neighborhood = neighborhood, 
r.textmining = textmining, r.database = database, r.combined_score = combined_score
' %>% cypher

'MATCH ()-[r:STRINGdb]-() RETURN count(r)' %>% cypher %>% unlist

```


```{r}
library(ade4)
# Statistics

total_node = "MATCH (g) return labels(g)[0] as node, count(g) as number_of_node" %>% cypher %>% as.data.frame()

names(total_node)[1] = "node"
names(total_node)[2] = "number_of_node"

par(oma=c(1,1,1,1)) # Make sure plot has enough space
par(mar=c(6,5,5,3) + 0.1)
barplot(total_node$number_of_node, names.arg = total_node$node, horiz = T, las=2, main="Number of node per label" )


total_edges = "MATCH ()-[r]->() return type(r) as relationship_name, count(r) as number_of_relationship" %>% cypher %>% as.data.frame()
names(total_edges)[1] = 'relationship'
names(total_edges)[2] = 'relationship_count'
par(mar = c(7, 4, 2, 2) + 0.2)
barplot(total_edges$relationship_count, names.arg = total_edges$relationship, horiz = T, las=2, col=rainbow(10), main="Number of relation per type of relation")

gene_link = "MATCH (g:Gene)-[s:STRINGdb]->(g1:Gene) return g.bnumber as node, count(g) as number_of_node" %>% cypher %>% as.data.frame()

g = cut(gene_link$value.1, breaks = 4) %>% as.data.frame()
plot(g, main="Gene relationship distribution ", ylab = "Number of genes", xlab = 'Number of links between gene', col=rainbow(10))

process_GO = "match (g:GOTerm) with g.namespace as name, count(g.namespace) as cnt return name, cnt " %>% cypher %>% as.data.frame()

names(process_GO)[1] = "category"
names(process_GO)[2] = "count"
process_GO

par(oma=c(2,2,2,2))
par(mar=c(6,5,5,3) + 0.1)

barplot(process_GO$count, names.arg = process_GO$category, main="Distribution of GO process", ylab = "Number of process", xlab = 'Process name')
par(mar(c))


interpro_data = "match (i:InterPro)-[r:harbored_by]->(g:Gene) with i.id as id, count(r) as cnt return id, cnt ORDER BY cnt desc LIMIT 10" %>% cypher %>% as.data.frame()
names(interpro_data)[1] = "protein_family_number"
names(interpro_data)[2] = "count"
barplot(interpro_data$count, names.arg = interpro_data$protein_family_number, col=rainbow(10), horiz = T, las=2, main="InterPro Protein family distribution", ylab = mtext("Family id\n\n\n\n\n\n\n\n\n\n\n\n\n", side = 2, line = 3, las=1), xlab = 'Number of genes associated')

par(oma=c(1,6,1,1))
par(mar=c(6,5,5,3) + 0.1)
key_data = "MATCH (k:Keyword)-[r:describes]->(g:Gene) with k.keyword as keyword, count(r) as cnt RETURN keyword, cnt ORDER BY cnt DESC LIMIT 10" %>% cypher %>% as.data.frame()
names(key_data)[1] = "keyword"
names(key_data)[2] = "keyword_count"
barplot(key_data$keyword_count, names.arg = key_data$keyword, col=rainbow(10), horiz = T, las=2, main="10 most common keywords distribution", ylab = mtext("Keyword name\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", side = 2, line = 3, las=1), xlab = 'Number of keywords associated with genes')

par(oma=c(1,10,1,1))
par(mar=c(6,5,5,3) + 0.1)
path_data = "MATCH (k:Pathways)-[r:requires]->(g:Gene) with k.id as path, count(r) as cnt RETURN path, cnt ORDER BY cnt DESC LIMIT 10" %>% cypher %>% as.data.frame()
names(path_data)[1] = "Pathways"
names(path_data)[2] = "Pathway_count"
barplot(path_data$Pathway_count, names.arg = path_data$Pathways, col=rainbow(10), horiz = T, las=2, main="Distribution of the 10 most common \npathways ", ylab = mtext("Pathway id\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", side = 2, line = 3, las=1), xlab = 'Number of pathways associated with genes')

par(oma=c(1,3,1,1))
par(mar=c(6,5,5,3) + 0.1)
TU_data = "MATCH (k:TU)-[r:harbors]->(g:Gene) with k.id as TU, count(r) as cnt RETURN TU, cnt ORDER BY cnt DESC LIMIT 10" %>% cypher %>% as.data.frame()
names(TU_data)[1] = "TU"
names(TU_data)[2] = "TU_count"
barplot(TU_data$TU_count, names.arg = TU_data$TU, col=rainbow(10), horiz = T, las=2, main="Distribution of the 10 most common transcription units ", ylab = mtext("TU id\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", side = 2, line = 3, las=1), xlab = 'Number of transcription units associated with genes')

par(oma=c(1,2,1,1))
par(mar=c(6,5,5,3) + 0.1)
pub_data = "MATCH (p:PubMed)-[r:cites]->(g:Gene) with g.bnumber as num, count(r) as cnt RETURN num, cnt as citations ORDER BY cnt DESC LIMIT 10" %>% cypher %>% as.data.frame()
names(pub_data)[1] = "gene_bnumber"
names(pub_data)[2] = "citation_count"


barplot(pub_data$citation_count, names.arg = pub_data$gene_bnumber, col=rainbow(10), horiz = T, las=2, main="Distribution of the 10 most cited genes ", ylab = mtext("bnumber\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", side = 2, line = 3, las=1), xlab = 'Number of citation')

#STRINGdb_complete = "match (g:Gene)-[r:STRINGdb]->() return g.bnumber, r.coexpression, r.neighborhood, r.experimental, r.textmining, r.combined_score" %>% cypher %>% as.data.frame()
# too heavy for R
par(oma=c(0,0,0,0)) 
par(mar=c(5,4,4,2) + 0.1)
STRINGdb_complete = read.csv("data/total_string.csv")
plot = dudi.pca(STRINGdb_complete[2:6], scannf = F, nf = 2)
s.corcircle(plot$co, csub=1)
```

```{r}
library(STRINGdb)
confidence_threshold = 333
stringdb = STRINGdb$new(version='11.5', species=511145, score_threshold=confidence_threshold, input_directory='data')

# Comparing differents statistical methods 


most_annotated_gene = "MATCH (g:Gene)<-[r:annotates]-(o:GOTerm) with o.id as id, count(r) as cnt return id ORDER BY cnt DESC LIMIT 1" %>% cypher %>% unlist
most_annotated_gene

# Selecting only genes that are annotated by two genes 
gene_list_shared = paste("MATCH (o:GOTerm)-[r:annotates]->(g:Gene)<-[r1:annotates]-(o1:GOTerm{id:'",most_annotated_gene,"'}) return DISTINCT g.bnumber", sep="") %>% cypher %>% as.data.frame()

# Writing the set of genes to be enriched by STRINGdb
body = ""
for (i in 1:nrow(gene_list_shared)) {
  body = paste(body, (gene_list_shared$value[i]))
}
body %>% write("data/synth_set_shared.txt")

# STRINGdb enrichment

s1 = scan('data/synth_set_shared.txt', character()) 
s1.mapped = stringdb$mp(s1)
enrichment = stringdb$get_enrichment(s1.mapped)
enrichment = stringdb$get_enrichment(s1.mapped) %>% filter(str_detect(term, '\\bGO:\\b')) %>% write_csv('STRINGdb_shared.csv')

# Comparison between STRINGdb and our method

chi = read.csv('result_all_chi.tsv', sep = '\t', header = F)
cov = read.csv('result_all_cov.tsv', sep = '\t', header = F)
hyper = read.csv('result_all_hyper.tsv', sep = '\t', header = F)
binom = read.csv('result_all_binom.tsv', sep = '\t', header = F)

# Creating the dataframe containing the number of GOTerms identical between STRINdb and the method
# Normalized by the size of GOTerm found

df_count = subset(chi, V1 %in% enrichment$term) %>% nrow() %>% as.data.frame()
df_count$chi_count = subset(chi, V1 %in% enrichment$term) %>% nrow()/nrow(chi)
df_count = df_count[2:2]
df_count$cov_count = subset(cov, V1 %in% enrichment$term ) %>% nrow() /nrow(cov)
df_count$hyper_count = subset(hyper, V1 %in%  enrichment$term) %>% nrow()/nrow(hyper)
df_count$binom_count = subset(binom, V1 %in%  enrichment$term) %>% nrow()/nrow(binom)
df_count$workaround = 0
df_count = df_count %>% pivot_longer(!workaround, names_to = "method", values_to = 'count')
df_count = df_count[2:3]

df_count1$chi_count = df_count[1,]
barplot(df_count$count, names= df_count$method, main = "Normalized score of identity between \nmethod and STRINGdb")

```



